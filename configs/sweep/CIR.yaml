# @package hydra.sweeper
# Optuna sweep params for CIR trainer (used only with --multirun)

# params:
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.1, 0.7))"
#   trainer.method_args.cfg.mahal_quantile: "interval(0.3, 1)"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(1e-4, 1e-1))"
#   trainer.method_args.cfg.mahal_pow: "interval(0.0, 1.5)"
#   trainer.method_args.cfg.act_norm_pow: "interval(-0.5, 0.5)"

# quantization6
# params: 
#   # trainer.method_args.cfg.act_norm_pow: "interval(-0.5, 0.5)"
#   trainer.method_args.cfg.mahal_pow: "interval(0.1, 2.0)"
#   trainer.method_args.cfg.mahal_quantile: "interval(0.7, 1)"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(3e-4, 3e-2))"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.2, 1.0))"
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(3e-4, 3e-2))"
#   trainer.method_args.cfg.filter_pow: "interval(0.1, 1.0)"

# # decoupled_reg
# # pow and reg params seem overdetermined (kinda), so set pow to 1
# params: 
#   # trainer.method_args.cfg.mahal_pow: "interval(0.1, 2.0)"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(3e-4, 3e-2))"
#   # trainer.method_args.cfg.filter_pow: "interval(0.1, 1.0)"
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(3e-4, 3e-2))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.2, 1.0))"

# # top_collapse
# # pow and reg params seem overdetermined (kinda), so set pow to 1
# params: 
#   # trainer.method_args.cfg.mahal_pow: "interval(0.1, 2.0)"
#   # trainer.method_args.cfg.mahal_reg: "tag(log, interval(3e-4, 3e-2))"
#   trainer.method_args.cfg.filter_pow: "interval(-0.1, 1.0)"
#   trainer.method_args.cfg.act_collapse: "range(0, 30)"
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(3e-4, 3e-2))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.2, 1.0))"

# # no_project_to_mahal
# params: 
#   trainer.method_args.cfg.mahal_pow: "interval(0.1, 2.0)"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(3e-4, 3e-2))"
#   trainer.method_args.cfg.filter_pow: "interval(0.5, 1.0)"
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(3e-4, 3e-2))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   # trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.2, 1.0))"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.01, 0.2))"

# # train_from_layer
# params: 
#   trainer.method_args.cfg.filter_pow: "interval(0.6, 1.0)"
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(1e-2, 3e-1))"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(3e-4, 1e-2))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.4, 1.0))"
#   trainer.method_args.cfg.train_from_layer: "range(0, 5)"

# # train_from_layer fianl ranges:
# params:
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(5e-1, 20))"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(1e-3, 20e-3))"
#   trainer.method_args.cfg.quantile: "interval(0.85, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.3, 1.0))"
#   trainer.method_args.cfg.train_from_layer: "range(0, 3)"

# # per_text_quantile
# params: 
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(1, 10))"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(1e-3, 1e-2))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.4, 1.0))"

# # mlp_proj
# params: 
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(0.3, 10))"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(1e-4, 1e-2))"
#   trainer.method_args.cfg.mlp_reg: "tag(log, interval(1e-2, 1e0))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(0.4, 2.0))"

# mlp_proj_scaled_reg
# params:
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(0.03, 10))"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(1e-5, 1e-3))"
#   trainer.method_args.cfg.mlp_reg: "tag(log, interval(3e-4, 0.3))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(1, 10))"

# # mlp_filter
# params: 
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(0.1, 10))"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(1e-4, 3e-3))"
#   trainer.method_args.cfg.mlp_filter_reg: "tag(log, interval(0.01, 50))"
#   trainer.method_args.cfg.mlp_reg: "tag(log, interval(3e-2, 3))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.mlp_quantile: "interval(0.4, 1)"
#   trainer.method_args.cfg.unlearning_rate: "tag(log, interval(1, 100))"

# # kl
# params: 
#   trainer.method_args.cfg.filter_reg: "tag(log, interval(0.01, 10))"
#   trainer.method_args.cfg.mahal_reg: "tag(log, interval(1e-4, 3e-3))"
#   trainer.method_args.cfg.mlp_filter_reg: "tag(log, interval(0.001, 50))"
#   trainer.method_args.cfg.mlp_reg: "tag(log, interval(3e-3, 3))"
#   trainer.method_args.cfg.quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.mlp_quantile: "interval(0.0, 1)"
#   trainer.args.learning_rate: "tag(log, interval(1, 50))"

# # quick_mlp_reg
# params: 
#   trainer.args.learning_rate: "tag(log, interval(1, 3))"
#   # trainer.method_args.cfg.act_quantile: "interval(0.9, 1)"
#   trainer.method_args.cfg.act_reg: "tag(log, interval(5e-4, 5e-3))"
#   trainer.method_args.cfg.mlp_reg: "tag(log, interval(1e-3, 1))"

# # filter_before_cache
# params: 
#   trainer.args.learning_rate: "tag(log, interval(0.6, 3))"
#   trainer.method_args.cfg.act_quantile: "interval(0.8, 1)"
#   trainer.method_args.cfg.act_reg: "tag(log, interval(3e-4, 1e-2))"

params: 
  trainer.args.learning_rate: "tag(log, interval(0.6, 1))"
  trainer.method_args.cfg.act_quantile: "interval(0.9, 0.96)"
  trainer.method_args.cfg.act_reg: "tag(log, interval(1e-3, 8e-3))"
