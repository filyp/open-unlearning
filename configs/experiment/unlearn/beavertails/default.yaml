# @package _global_
 
defaults:
  - /trainer@relearning_trainer: finetune
  # - override /model: Llama-3.1-8B
  - override /model: Llama-3.2-3B
  # - override /model: Llama-3.2-1B
  # - override /model: MobileLLM-125M
  # - override /model: MobileLLM-600M
  # Delete inherited TOFU datasets from base data/unlearn.yaml
  - override /data/datasets@data.forget: null
  - override /data/datasets@data.retain: null
  - override /eval: null  # This disables the default TOFU evaluation
  - override /hydra/sweeper: optuna  # Use Optuna sweeper for --multirun

# task_name: ${model.pretrained_model_name_or_path}_${trainer.handler}

trainer:
  save_final_state: False
  args:
    report_to: wandb  # none|tensorboard|wandb|dvclive|comet_ml
    run_name: ${task_name}
    do_eval: False

eval:
  wikitext:
    handler: KLEvaluator
    dataset_name: wikitext
    disr_budget: 0.005
  beavertails:
    handler: LossEvaluator
    dataset_name: beavertails_safe

tokenizer_cfg:
  max_length: 128  # for our wmdp corpora it's more than enough

data:
  custom_loaders:
    - loader: beavertails
      category: animal_abuse
      eval_limit: 256
      target_limit: 1024
      retain_limit: 1024
      # eval_limit: 64
      # target_limit: 128
      # retain_limit: 128
      tokenizer: ${tokenizer_cfg}
    - loader: load_hf_and_tokenize
      dataset_name: wikitext
      hf_args:
        path: filypo/wikitext_16k
      limit: 128
      tokenizer: ${tokenizer_cfg}



# categories = [
#     "animal_abuse",
#     "child_abuse",
#     "controversial_topics,politics",
#     "discrimination,stereotype,injustice",
#     "drug_abuse,weapons,banned_substance",
#     "financial_crime,property_crime,theft",
#     "hate_speech,offensive_language",
#     "misinformation_regarding_ethics,laws_and_safety",
#     "non_violent_unethical_behavior",
#     "privacy_violation",
#     "self_harm",
#     "sexually_explicit,adult_content",
#     "terrorism,organized_crime",
#     "violence,aiding_and_abetting,incitement",
# ]



# relearning_trainer:
#   # default loadede from finetune.yaml
#   save_final_state: False
#   handler: FinetuneTrainer
#   args:
#     optim: adamw_8bit  # sgd | adamw_8bit | paged_adamw_8bit
#     learning_rate: 5e-6
#     lr_scheduler_type: constant
#     report_to: wandb  # none|tensorboard|wandb|dvclive|comet_ml
#     run_name: ${task_name}
# relearning_eval:
#   wmdp_low_mi:
#     handler: WMDPLLowMIEvaluator
#     eval_mcq: true
#     save_best_model: false
#     wikitext:
#       dataset_name: wikitext
#       batch_size: 16
#       num_batches: 8
#       tokenizer: ${tokenizer_cfg}
#       # KL Divergence budget
#       disr_budget: null  # null means training will not be terminated based on KL



# # Optuna sweeper config (activate with --multirun flag)
# hydra:
#   sweeper:
#     # hyperparam range loaded from /sweep config group in configs/trainer/*.yaml files
#     _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
#     storage: ${oc.env:OPTUNA_STORAGE_URL}
#     study_name: ${task_name}
#     direction: minimize
#     n_trials: 50
#     n_jobs: 1
#     sampler:
#       _target_: optuna.samplers.TPESampler
#       seed: ${trainer.args.seed}  # deterministic
#       # seed: ${now:%f}  # microseconds timestamp - unique per job, to avoid parallel searches duplicating trials
 
