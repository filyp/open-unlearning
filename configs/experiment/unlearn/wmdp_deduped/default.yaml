# @package _global_

defaults:
  - override /eval: null  # This disables the default TOFU evaluation
  # - override /model: Llama-3.2-3B-Instruct
  # - override /model: Llama-3.2-3B
  - override /model: Llama-3.2-1B

# mode: wmdp_deduped  # needs to be overriden with CLI

trainer:
  args:
    report_to: none  # suppress the default wandb reporting in the trainer
    # report_to: wandb
    # run_name: ${task_name}  # that's the default

tokenizer_cfg:
  max_length: 128
  padding: true
  truncation: true


data:
  custom_loaders:
    wmdp_bio_deduped:
      dataset: bio
      use_dev_split: false
      eval_on_all_questions: false
      num_examples_per_question: 3
      tokenizer: ${tokenizer_cfg}
    wikitext:
      load_as: wikitext
      wikitext_batch_size: 16
      tokenizer: ${tokenizer_cfg}
    load_hf:  # fineweb bio dataset
      load_as: retain
      limit: 2500
      tokenizer: ${tokenizer_cfg}
      hf_args:
        path: m-a-p/FineFineWeb
        split: train
        data_files:
          # for bio: (just this smallest parquet file will be enough)
          - biology/biology_000849.jsonl
          # for cyber:
          # - computer_science_and_technology/computer_science_and_technology_000000.jsonl

eval:
  deduped:
    handler: WMDPDedupedEvaluator
    num_eval_batches: 16
    # * uncomment to use wandb:
    wandb:
      project: unlearning|src|main_runner.py
      group: nov_test
      name: ${task_name}

# relearning:
#   lr: 1e-5  # even with 1e-6, wikitext_loss has this Nike shape
#   num_epochs: 3
#   relearn_batch_size: 32  # can be higher because batches are usually shorter
#   relearning_eval:
#     handler: WMDPDedupedEvaluator
#     num_eval_batches: 16
#     # * uncomment to use wandb:
#     wandb:
#       project: ret_unlearning|src|main_runner.py
#       group: nov_test
#       name: ${task_name}


# model_id: meta-llama/Llama-3.2-1B
# model_id: meta-llama/Llama-3.2-3B
# model_id: meta-llama/Llama-3.1-8B

# default experiment config
# loss_budget: 1.001
# max_num_epochs: 200

# retraining_rate: 1e-5  # even with 1e-6, wikitext_loss has this Nike shape