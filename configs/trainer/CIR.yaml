# could load values from finetune.yaml like other trainer configs, but we are too different
defaults:
  # - finetune
  - /sweep: CIR  # Load CIR-specific sweep params (used with --multirun)

handler: CIR
args:
  optim: sgd
  lr_scheduler_type: constant
  learning_rate: 0.15
  per_device_train_batch_size: 16
  # # per_device_eval_batch_size: 16

method_args:
  cfg:
    act_quantile: 0.9
    
    act_pcs_to_use: 400
    grad_pcs_to_use: 24

    target_modules:
      - gate_proj
      - up_proj
      # - down_proj
    
    forget_loss: mlp_breaking
    # forget_loss: null

    # ! unused params
    # mlp_quantile: 0.5  # useful on later layers, but very slightly
    # mlp_reg: 3e-3  # no effect, at least for WMDP; todo test it on toxicity
    # layer_range: [0, 10]
    ## Llama-3.2-1B has 16 layers
    ## MobileLLM-125M has 30 layers

    # # ! retaining params
    # retaining_rate: 2e-1
    # cb_retaining_layers: [6, 9, 12, 15]  # for 8B use [12]
    
    # ! legacy params
    # mlp_floor: 0.0
    # cir_niter: 16
    # cut_off_tokens: 1
    # pca_every_n: 1
    # act_proj_num: 10
    # grad_proj_num: 0
    # train_from_layer: 0
    # mahal_pow: 1.0
    # filter_pow: 1.0
    # act_collapse: 0
    # filter_reg: 10
    # project_to_mahal: True
    # mlp_filter_reg: 0.002
    # mlp_out_norm_pow: 0.0
    # grad_filt_pow: 0.0
    # cb_retaining_pow: 1.0