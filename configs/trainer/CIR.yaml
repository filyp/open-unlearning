# could load values from finetune.yaml like other trainer configs, but we are too different
defaults:
  # - finetune
  - /sweep: CIR  # Load CIR-specific sweep params (used with --multirun)

handler: CIR
args:
  seed: 42
  output_dir: ${paths.output_dir}
  do_train: True
  do_eval: False
  eval_on_start: True
method_args:
  cfg:
    max_num_epochs: 16
    mlp_floor: 0.0
    layer_range: [4, 7]  # for 8B use [6, 12]
    quantile: 0.93
    mlp_quantile: 0.9

    mahal_reg: 1e-3
    filter_reg: 1  # 1-10 works fine
    mlp_filter_reg: 5
    mlp_reg: 0.2

    project_to_mahal: True
    filter: True

    unlearning_rate: 5
    target_modules:
      - gate_proj
      - up_proj
      # - down_proj
    train_batch_size: 22
    retain_batch_size: 12  # lowered from 16 because it caused OOM with GA

    # ! retaining params
    # retaining_rate: 3e-4  # 1e-3 is fine too, but let's be safe and slow
    retaining_rate: 0
    cb_retaining_layers: [7]  # for 8B use [12]
    cb_retaining_pow: 1.0
    
    # ! legacy params
    # cir_niter: 16
    # cut_off_tokens: 1
    # pca_every_n: 1
    # act_proj_num: 10
    # grad_proj_num: 0
    # train_from_layer: 0
    # mahal_pow: 1.0
    # filter_pow: 1.0
    # act_collapse: 0