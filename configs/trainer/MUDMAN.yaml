defaults:
  - finetune
  # - /hydra/sweeper: MUDMAN

handler: MUDMAN
args:
  optim: sgd  # adam underperforms
  # learning_rate: 0.005
  learning_rate: 0.015
  lr_scheduler_type: constant
  gradient_accumulation_steps: 1  # must be one, because CIR modifies gradients in-place


  per_device_train_batch_size: 6  # todo change back later

method_args:
  cfg:

    target_modules:
      - gate_proj
      - up_proj
    
    train_first_layers: 0.75
    # train_first_layers: 1
    
    retain_momentum: 0.9
    